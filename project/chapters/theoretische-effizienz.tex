\chapter{Asymptotische Analyse als Effizienzangabe}
\label{cha:asymptotic-analysis}

Dieses Kapitel beschäftigt sich mit der Frage, wie die theoretische Effizienz (lt. der Definition in \prettyref{sec:algo-effizienz}) eines Algorithmus ermittelt werden kann.

Dafür werden in \prettyref{sec:einfache-operationen} und \prettyref{sec:asymptotic-cases} die grundlegende Einheit der theoretischen Effizienz und einige untergeordnete Unterteilungen erläutert. Ein Werkzeug für die vereinfachende Darstellung der Effizienz wird in \prettyref{sec:asymptotic-analysis} gegeben.

In \prettyref{sec:praxisnahe-komplexitäten} werden einige oft vorkommende Wachstumsraten der Effizienz als Funktion $T(n)$ dargestellt.

Schlussendlich werden in \prettyref{sec:te-ermittlung} einige Pseudocode-Ausschnitte beispielhaft hinsichtlich ihrer Effizienz analysiert, um den Ermittlungsprozess der theoretischen Effizienz darzustellen.

\section{Einfache Operationen}
\label{sec:einfache-operationen}

Die Werte von $T(n)$, also die theoretische Effizienz eines Algorithmus auf einer Eingabemenge der Größe $n$, sind im Kontext der theoretischen Analyse die Anzahl der \enquote{einfachen Operationen} oder \enquote{Schritten}, welche für diese Eingabemenge ausgeführt werden müssen (vgl. \cite[25]{clrs2001} und \cite[18f]{hsr1997}). \enquote{Einfache Operationen} sind hier jene Operationen, deren benötigte Zeit unabhängig von den Operanden ist, beispielsweise arithmetische Grundrechenarten und Vergleiche (vgl. \cite[55]{sha2011}). Durch diese Einschränkung kann die Anzahl der einfachen Operationen als stellvertretend zur tatsächlichen, sonst experimentell zu ermittelnden Laufzeit gesehen werden (vgl. \cite[55]{sha2011}).

\section{Szenarien}
\label{sec:asymptotic-cases}

Es wird zwi\-schen der theoretischen Effizienz im \enquote{günstigsten}, \enquote{ungünstigsten}, und \enquote{durchschnittlichen Szenario bzw. \emph{Fall}} unterschieden, alle beziehen sich auf die Beschaffenheit der Eingangsmenge (vgl. \cite[28]{hsr1997}).

Zur Veranschaulichung dieser Unterscheidung ist ein Suchalgorithmus, $\proc{Sequential-Search}$ (vgl. \cite[396]{taocp3}), zu betrachten, der eine Liste nach einem Element durchsucht und terminiert, nachdem er es gefunden hat.

\begin{codebox}
    \Procname{$\proc{Sequential-Search}(A, \id{value})$}
    \li \For $i \gets 1$ \To $\attrib{A}{length}$
    \li     \Do
                \If $A[i] \isequal \id{value}$
    \li             \Then
                        \Return $i$
                    \End
            \End
    \li \Return $0$
\end{codebox}

Der günstigste Fall für einen solchen Algorithmus tritt auf, wenn die Eingangsmenge eine Liste ist, in der das gesuchte Element an der ersten Position steht. In diesem Fall wird nur ein Vergleich ausgeführt, die Laufzeit ist kurz. Steht das gesuchte Element jedoch an der letzten Position, so werden $\attrib{A}{length}$ beziehungsweise $n$ Vergleiche ausgeführt, eine solche Menge führt zum ungünstigsten Fall. Unter der Annahme, dass die Elemente von $A$ gleichmäßig verteilt sind, führt der Algorithmus durchschnittlich $n/2$ Vergleiche aus, dies ist der durchschnittliche Fall (vgl. \cite[59]{sha2011}).

\section{Asymptotische Analyse}
\label{sec:asymptotic-analysis}

Zur einleitenden Frage, wie die asymptotische Analyse zu beschreiben sei, schreibt \citeauthor{bru1958} in \citetitle{bru1958}:

\blockquote[{\cite[1]{bru1958}}]{It often happens that we want to evaluate a certain number, defined in a certain way, and that the evaluation involves a very large number of operations so that the direct method is almost prohibitive. In such cases we should be very happy to have an entirely different method for finding information about the number, giving at least some useful approximation to it. [\ldots] A situation like this is considered to belong to asymptotics.}

Die Ermittlung der exakten Anzahl an einfachen Operationen, die ein Algorithmus mit einer gewissen Eingabemenge benötigt, ist üblicherweise nicht lohnenswert (vgl. \cite[28]{hsr1997}) oder sogar unmöglich (vgl. \cite[37]{meh1984}) -- gemäß \citeauthor{bru1958} ist in diesem Fall eine Ermittlung mithilfe asymptotischer Methoden ratsam. Diese \enquote{certain number}, die im obigen Zitat erwähnt wird, ist im gegebenen Kontext demzufolge $T(n)$, die Information, die es zu finden gilt, ist die \emph{Größenordnung des Wachstums} (auch \emph{Wachstumsrate}) der Funktion $T$ (vgl. \cite[63]{sha2011}).

Diese Größenordnung des Wachstums bei steigenden bzw. großen Werten von $n$ wird in der einschlägigen Literatur oft in der $O$-Notation angeschrieben (vgl. \cite[2]{ahu1974}, \cite[107]{taocp1}, \cite[29]{hsr1997}, \ldots).

Kann die Effizienz eines Algorithmus als
\begin{equation}\label{eq:unneccessary-constants}
    T(n) = an^2 + bn + c
\end{equation}
ausgedrückt werden (wobei $a$, $b$ und $c$ beliebige von $n$ unabhängige Konstanten sind), so kann \eqref{eq:unneccessary-constants} mithilfe dieser $O$-Notation zu
\begin{equation}\label{eq:basic-o-notation}
    T(n) = O(n^2)
\end{equation}
vereinfacht werden.

Diese Vereinfachung verleiht dem Umstand Ausdruck, dass hier nur der Term $n^2$ von Interesse ist. Terme niederer Ordnung wie $bn$ und $c$, und konstante Faktoren wie $a$ werden bei großem bzw. wachsendem $n$ relativ bedeutungslos (vgl. \cite[28]{clrs2001}), und können so \enquote{weggelassen werden}. Die $O$-Notation ersetzt also die Kenntnis einer Zahl durch das Wissen, dass eine solche Zahl existiert (vgl. \cite[3]{bru1958}).

Das ist der Kern der asymptotischen Analyse in diesem Kontext: Die Untersuchung eines Algorithmus für große $n$ und die damit einhergehende Möglichkeit der Vereinfachung von Algorithmen (vgl. \cite[63]{sha2011}).

\input{tables/unimportant-constants}

\prettyref{tab:unimportant-constants} veranschaulicht diese Vernachlässigbarkeit von Termen niederer Ordnung und von konstanten Faktoren bei großem $n$.

\paragraph{Komplexität} Die Funktion $T(n)$, also die Laufzeit eines Algorithmus in Abhängigkeit der Eingabegröße $n$, wird auch \emph{Zeitkomplexität} des Algorithmus genannt. Das asymptotische Verhalten von $T(n)$ für große Werte von $n$ wird \emph{asymptotische Zeitkomplexität} genannt (vgl. \cite[2]{ahu1974}). Im Folgenden ist mit \emph{Komplexität} eines Algorithmus die asymptotische Zeitkomplexität des Algorithmus gemeint.

Ein Algorithmus in $O(n^2)$ hätte demzufolge eine \enquote{Komplexität von $n^2$} -- für große $n$ kann seine Laufzeit durch die Funktion $f(n) = n^2$ beschrieben werden.

\paragraph{Definition der $O$-Notation}

Genauer beschreibt $O(g(n))$ für eine gegebene Funktion $g(n)$ (im Fall von \eqref{eq:basic-o-notation} also $g(x) = n^2$) die Menge von Funktionen (vgl. \cite[37]{meh1984})
%
\begin{equation*}
    O(g(n))=\left\{\ 
        f(n)\ :\ \parbox{.6\textwidth}{
            Es existieren positive Konstanten $c$ und $n_0$ derart, dass $0 \leq f(n) \leq c \cdot g(b)$ für alle $n \geq n_0$.
        }
    \ \right\}.
\end{equation*}

Die $O$-Notation definiert eine \enquote{asymptotische obere Schranke} (vgl. \cite[64]{sha2011}): Der Wert der Funktion $f(n)$ ist kleiner oder gleich dem Wert der Funktion $c \cdot g(n)$ für alle $n \geq n_0$.

\section{Praxisnahe Komplexitäten}
\label{sec:praxisnahe-komplexitäten}

Einige der in diesem Kontext am häufigsten vorkommenden Komplexitäten sind $n^2$, $\log n$ und $n \log n$\footnotemark (vgl. \cite[38]{hsr1997}). Anders ausgedrückt sind also viele Sortieralgorithmen in $O(n^2)$, $O(\log n)$ oder $O(n \log n)$. \prettyref{tab:common-growth-rates} veranschaulicht diese Komplexitäten anhand von Funktionswerten für diverse Werte von $n$.

\footnotetext{Alle in dieser Arbeit behandelten Sortieralgorithmen haben eine dieser Komplexitäten.}

Der in \prettyref{tab:common-growth-rates} ebenfalls dargestellte \emph{Quotient} der jeweiligen Funktionswerte und $n$ zeigt das Verhältnis eines Algorithmus, dessen Laufzeit linear wächst, zu einem Algorithmus, dessen Laufzeit durch eine der dargestellten Komplexitäten zu beschreiben ist. So ist ein Algorithmus in $O(\log n)$ für $n = 256$ fast 50-fach schneller als ein Algorithmus in $O(n)$.

Anzumerken ist jedoch, dass die $O$-Notation die \emph{asymptotische} Wachstumsrate darstellt (bzw. die Komplexität eines Algorithmus als asymptotisches Verhalten zu sehen ist). So kann etwa ein Algorithmus in $O(n^2)$ für kleine $n$ schneller als ein Algorithmus in $O(n)$ sein.

\input{tables/common-growth-rates.tex}

%Zu beachten ist jedoch, dass eine Funktion mit asymptotisch großer Wachstumsrate bei kleineren Werte von $n$ auch kleinere Werte als eine Funktion mit asymptotisch kleiner Wachstum haben kann (vgl. \cite[58]{sha2011}). Abbildung \ref{fig:growth-rates-example} zeigt diesen Umstand insbesondere am Beispiel von Funktionen mit den Wachstumsraten $2n^2$ und $20n$.

%Abbildung \ref{fig:growth-rates-example} demonstriert ebenfalls den enormen Unterschied in der theoretischen Effizienz (und dadurch im theoretischen Ressourcenkonsum) den die Wachstumsrate ausmacht.

\section{Ermittlung der theoretischen Effizienz}
\label{sec:te-ermittlung}

Im folgenden Abschnitt werden Komplexitätsanalysen diverser Pseudocode-Ausschnitte (vgl. \cite[69]{sha2011}) präsentiert.

\begin{codebox}
    \li $a = 2 \cdot A[i]$          \label{ln:ex1-setup}
    \li \If $a \leq \frac{b}{4}$    \label{ln:ex1-if}
    \li     \Then
                \Return $a$         \label{ln:ex1-return}
            \End
\end{codebox}

Alle vorkommenden Operationen -- die Zuweisung, Multiplikation und der Arrayzugriff in \prettyref{ln:ex1-setup}; der Vergleich und die Division in \prettyref{ln:ex1-if}; die \Return-Anweisung in \prettyref{ln:ex1-return} -- sind einfache Operationen (siehe \prettyref{sec:einfache-operationen}). Folglich ist der Ausschnitt in $O(1)$, seine Laufzeit ist konstant und unabhängig von den behandelten Werten.

\begin{codebox}
    \li $s \gets 0$                 \label{ln:ex2-outer-assignment}
    \li \For $j \gets 0$ \To $n$    \label{ln:ex2-for}
    \li     \Do
                $s \gets s + 1$     \label{ln:ex2-inner-assignment}
            \End
\end{codebox}

\prettyref{ln:ex2-outer-assignment} ist in $O(1)$. Die \For-Schleife in \prettyref{ln:ex2-for} wird $n$ Mal wiederholt. Nachdem der Körper der Schleife, \prettyref{ln:ex2-inner-assignment}, in $O(1)$ ist, ist die Schleife in $O(n)$. Demzufolge ist der gesamte Ausschnitt in $O(n)$.

\begin{codebox}
    \li $s \gets 0$                     \label{ln:ex3-setup}
    \li \For $j \gets 0$ \To $n$        \label{ln:ex3-outer-for}
    \li     \Do
                \For $i \gets 0$ \To $j$\label{ln:ex3-inner-for}
    \li             \Do
                        $s \gets s + 1$ \label{ln:ex3-inner-for-body}
                    \End
            \End
    \li \For $k \gets 0$ \To $n$        \label{ln:ex3-second-for}
    \li     \Do
                $A[k] \gets \frac{s}{k}$\label{ln:ex3-second-for-body}
            \End
\end{codebox}

Dieser Ausschnitt ist ein Beispiel eines etwas komplexeren Algorithmus mit drei \For-Schleifen.

Die erste Zeile ist in $O(1)$. Die letzte Schleife in \prettyref{ln:ex3-second-for} ist, wie im vorhergehenden Beispiel, in $O(n)$, nachdem ihr Körper in \prettyref{ln:ex3-second-for-body} in $O(1)$ ist (auch Zuweisungen in Arrays sind einfache Operationen).

Die erste Schleife in \prettyref{ln:ex3-outer-for} läuft $n$ Mal, bei jeder Iteration wächst ihr Schleifenzähler $j$ um 1. Wie oft die \enquote{innere} Schleife in \prettyref{ln:ex3-inner-for} läuft, ist abhängig von $j$: Für $j = 0$ läuft sie nicht, für $j = 1$ läuft sie einmal, für $j = 2$ läuft sie zweimal, \ldots, für $j = n$ läuft sie $n$ Mal. Nachdem gilt, dass (vgl. \cite{cf2007})
\begin{equation*}
    \sum_{i = 0}^{n} i = \frac{n(n - 1)}{2} = O(n^2)
\end{equation*}
und die Komplexität des Körpers der inneren Schleife in $O(1)$ ist, ist das gesamte Schleifenkonstrukt in $O(n^2)$.

Damit ist der Ausschnitt also in $O(1) + O(n) + O(n^2)$, was zu $O(n^2)$ vereinfacht werden kann.

\ifndef{vwarelease}{
    \paragraph{Andere Berechnungsmodelle} \cite{ahu1974} (\citetitle{ahu1974}) \emph{Nur als Notiz, falls noch Zeit für nähere Ausführungen bleibt.}
}{}
