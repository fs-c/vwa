\chapter{Asymptotische Analyse als Effizienzangabe}
\label{cha:asymptotic-analysis}

Dieses Kapitel beschäftigt sich mit der Frage, wie die theoretische Effizienz (lt. der Definition in \prettyref{sec:algo-effizienz}) eines Algorithmus ermittelt werden kann.

Zu diesem Zweck wird in \prettyref{sec:asymptotic-cases} eine Unterscheidung basierend auf der Beschaffenheit der Eingangsmenge etabliert, und in \prettyref{sec:asymptotic-analysis} ein Werkzeug für vereinfachende Darstellung von $T(n)$, die $O$-Notation, gegeben.

In \prettyref{sec:praxisnahe-komplexitäten} werden einige oft vorkommende Wachstumsraten von $T$ dargestellt.

\section{Einfache Operationen}

Die Werte von $T(n)$, also die theoretische Effizienz eines Algorithmus auf einer Eingabemenge der Größe $n$, sind im Kontext der theoretischen Analyse die Anzahl der \enquote{einfachen Operationen} oder \enquote{Schritten} welche für diese Eingabemenge ausgeführt werden müssen (vgl. \cite[25]{clrs2001} und \cite[18f]{hsr1997}). \enquote{Einfache Operationen} sind hier jene Operationen deren benötigte Zeit unabhängig von den Operanden ist, beispielsweise arithmetische Grundrechenarten und Vergleiche (vgl. \cite[55]{sha2011}). Durch diese Einschränkung kann die Anzahl der einfachen Operationen als stellvertretend zur tatsächlichen, sonst experimentell zu ermittelnden Laufzeit gesehen werden (vgl. \cite[55]{sha2011}).

\section{Günstigster, ungünstigster und durchschnittlicher Fall}
\label{sec:asymptotic-cases}

Es wird zwi\-schen der theoretischen Effizienz im \enquote{günstigsten}, \enquote{ungünstigsten}, und \enquote{durchschnittlichen Fall} unterschieden, alle beziehen sich auf die Beschaffenheit der Eingangsmenge (vgl. \cite[28]{hsr1997}).

Zur Veranschaulichung dieser Unterscheidung ist ein einfacher Suchalgorithmus, $\proc{Sequential-Search}$ (vgl. \cite[396]{taocp3}), zu betrachten der eine Liste nach einem Element durchsucht und terminiert nachdem er es gefunden hat.

\begin{codebox}
    \Procname{$\proc{Sequential-Search}(A, \id{value})$}
    \li \For $i \gets 1$ \To $\attrib{A}{length}$
    \li     \Do
                \If $A[i] \isequal \id{value}$
    \li             \Then
                        \Return $i$
                    \End
            \End
    \li \Return $0$
\end{codebox}

Der günstigste Fall für einen solchen Algorithmus tritt auf wenn die Eingangsmenge eine Liste ist, in der das gesuchte Element an der ersten Position steht. In diesem Fall wird nur ein Vergleich ausgeführt, die Laufzeit ist kurz. Steht das gesuchte Element jedoch an der letzten Position so werden $\attrib{A}{length}$ beziehungsweise $n$ Vergleiche ausgeführt, eine solche Menge führt zum ungünstigste Fall. Unter der Annahme, dass die Elemente von $A$ gleichmäßig verteilt sind, führt der Algorithmus durchschnittlich $n/2$ Vergleiche aus, dies ist der durchschnittliche Fall (vgl. \cite[59]{sha2011}).

\section{Asymptotische Analyse}
\label{sec:asymptotic-analysis}

Zur einleitenden Frage, wie die asymptotische Analyse zu beschreiben sei, schreibt \citeauthor{bru1958} in \citetitle{bru1958}:

\blockquote[{\cite[1]{bru1958}}]{It often happens that we want to evaluate a certain number, defined in a certain way, and that the evaluation involves a very large number of operations so that the direct method is almost prohibitive. In such cases we should be very happy to have an entirely different method for finding information about the number, giving at least some useful approximation to it. [\ldots] A situation like this is considered to belong to asymptotics.}

Die Ermittlung der exakten Anzahl an einfachen Operationen die ein Algorithmus mit einer gewissen Eingabemenge benötigt ist üblichweise nicht lohnenswert (vgl. \cite[28]{hsr1997}) oder sogar unmöglich (vgl. \cite[37]{meh1984}) -- gemäß \citeauthor{bru1958} ist in diesem Fall eine Ermittlung mithilfe asymptotischer Methoden ratsam.. Diese \enquote{certain number} die im obigen Zitat erwähnt wird ist im gegebenen Kontext demzufolge $T(n)$, die Information die es zu finden gilt ist die \emph{Größenordnung des Wachstums} (auch \emph{Wachstumsrate}) der Funktion $T$ (vgl. \cite[63]{sha2011}).

Diese Größenordnung des Wachstums bei steigendem bzw. großen Werten von $n$ wird in der einschlägigen Literatur oft in der $O$-Notation angeschrieben (vgl. \cite[2]{ahu1974}, \cite[107]{taocp1}, \cite[29]{hsr1997}, \ldots).

Kann die Effizienz eines Algorithmus als
\begin{equation}\label{eq:unneccessary-constants}
    T(n) = an^2 + bn + c
\end{equation}
ausgedrückt werden (wobei $a$, $b$ und $c$ beliebige von $n$ unabhängige Konstanten sind), so kann \eqref{eq:unneccessary-constants} mithilfe dieser $O$-Notation zu
\begin{equation}\label{eq:basic-o-notation}
    T(n) = O(n^2)
\end{equation}
vereinfacht werden.

Diese Vereinfachung verleiht dem Umstand Ausdruck, dass hier nur der Term $n^2$ von Interesse ist. Terme niederer Ordnung ($bn$ und $c$) und konstante Faktoren ($a$) werden bei großem bzw. wachsendem $n$ relativ bedeutungslos (vgl. \cite[28]{clrs2001}), und können so \enquote{weggelassen werden}. Die $O$-Notation ersetzt also die Kenntnis einer Zahl durch das Wissen, dass eine solche Zahl existiert (vgl. \cite[3]{bru1958}).

Das ist der Kern der asymptotischen Analyse in diesem Kontext: Die Untersuchung eines Algorithmus für große $n$ und die damit einhergehende Möglichkeit der Vereinfachung von Algorithmen (vgl. \cite[63]{sha2011}).

\input{tables/unimportant-constants}

\prettyref{tab:unimportant-constants} veranschaulicht diese Vernachlässigbarkeit von Termen niederer Ordnung und von konstante Faktoren bei großem $n$.

\paragraph{Komplexität} Die Funktion $T(n)$, also die Laufzeit eines Algorithmus in Abhängigkeit der Eingabegröße $n$ wird auch \emph{Zeitkomplexität} des Algorithmus genannt. Das asymptotische Verhalten von $T(n)$ für große Werte von $n$ wird \emph{asymptotische Zeitkomplexität} genannt (vgl. \cite[2]{ahu1974}). Im folgenden ist mit \emph{Komplexität} eines Algorithmus die asymptotische Zeitkomplexität des Algorithmus gemeint.

Ein Algorithmus in $O(n^2)$ hätte demzufolge eine \enquote{Komplexität von $n^2$} -- für große $n$ kann seine Laufzeit durch die Funktion $f(n) = n^2$ beschrieben werden.

\paragraph{Definition der $O$-Notation}

Genauer beschreibt $O(g(n))$ für eine gegebene Funktion $g(n)$ (im Fall von \eqref{eq:basic-o-notation} also $g(x) = n^2$) die Menge von Funktionen (vgl. \cite[37]{meh1984})
%
\begin{equation*}
    O(g(n))=\left\{\ 
        f(n)\ :\ \parbox{.6\textwidth}{
            es existieren positive Konstanten $c$ und $n_0$ derart, dass $0 \leq f(n) \leq c \cdot g(b)$ für alle $n \geq n_0$
        }
    \ \right\}
\end{equation*}

Die $O$-Notation definiert eine \enquote{asymptotische obere Schranke} (vgl. \cite[64]{sha2011}): Der Wert der Funktion $f(n)$ ist kleiner oder gleich dem Wert der Funktion $c \cdot g(n)$ für alle $n \geq n_0$.

\section{Praxisnahe Komplexitäten}
\label{sec:praxisnahe-komplexitäten}

Einige der in der in diesem Kontext am häufigsten vorkommenden Komplexitäten sind $n^2$, $\log n$ und $n \log n$\footnotemark (vgl. \cite[38]{hsr1997}). (Anders gesagt sind also viele Sortieralgorithmen in $O(n^2)$, $O(\log n)$ oder $O(n \log n)$.) \prettyref{tab:common-growth-rates} veranschaulicht diese Komplexitäten anhand von Funktionswerten für diverse Werte von $n$.

\footnotetext{Alle in dieser Arbeit behandelten Sortieralgorithmen haben eine dieser Komplexitäten.}

Der in \prettyref{tab:common-growth-rates} ebenfalls dargestellte \emph{Quotient} der jeweiligen Funktionswerte und $n$ zeigt das Verhältnis eines Algorithmus dessen Laufzeit linear wächst, zu einem Algorithmus dessen Laufzeit durch eine der dargestellten Komplexitäten zu beschreiben ist. So ist ein Algorithmus in $O(\log n)$ für $n = 256$ fast 50-fach schneller als ein Algorithmus in $O(n)$.

Anzumerken ist jedoch, dass die $O$-Notation die \emph{asymptotische} Wachstumsrate darstellt (bzw. die Komplexität eines Algorithmus als asymptotisches Verhalten zu sehen ist). So kann etwa ein Algorithmus in $O(n^2)$ für kleine $n$ schneller als ein Algorithmus in $O(n)$ sein.

\emph{In diesem Kontext anmerken, dass $O$-Notation die \emph{asymptotische Wachstumsrate} darstellt -- $O(n^2)$ kann für kleine $n$ schneller als $O(n)$ sein.}

\input{tables/common-growth-rates.tex}

%Zu beachten ist jedoch, dass eine Funktion mit asymptotisch großer Wachstumsrate bei kleineren Werte von $n$ auch kleinere Werte als eine Funktion mit asymptotisch kleiner Wachstum haben kann (vgl. \cite[58]{sha2011}). Abbildung \ref{fig:growth-rates-example} zeigt diesen Umstand insbesondere am Beispiel von Funktionen mit den Wachstumsraten $2n^2$ und $20n$.

%Abbildung \ref{fig:growth-rates-example} demonstriert ebenfalls den enormen Unterschied in der theoretischen Effizienz (und dadurch im theoretischen Ressourcenkonsum) den die Wachstumsrate ausmacht.

\section{Ermittlung}

Im folgenden Abschnitt werden Komplexitätsanalysen diverser Pseudocode-Ausschnitte präsentiert.

\begin{codebox}
    \li $a = 2 \cdot A[i]$
    \li \If $a \leq \frac{b}{4}$
    \li     \Then
                \Return $a$
            \End
\end{codebox}

\begin{codebox}
    \li $s \gets 0$
    \li \For $j \gets 0$ \To $n$
    \li     \Do
                $s \gets s + 1$
            \End
\end{codebox}

\begin{codebox}
    \li $s \gets 0$
    \li \For $j \gets 0$ \To $n$
    \li     \Do
                \For $i \gets 0$ \To $j$
    \li             \Do
                        $s \gets s + 1$
                    \End
            \End
    \li \For $k \gets 0$ \To $\attrib{A}{length}$
    \li     \Do
                $A[k] \gets \frac{s}{k}$
            \End
\end{codebox}

\paragraph{Andere Berechnungsmodelle} \cite{ahu1974} (\citetitle{ahu1974})
